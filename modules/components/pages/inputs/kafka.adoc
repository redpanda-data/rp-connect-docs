= kafka
// tag::single-source[]
:type: input
:status: stable
:categories: ["Services"]

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Connects to Kafka brokers and consumes one or more topics.


[tabs]
======
Common::
+
--

```yml
# Common configuration fields, showing default values
input:
  label: ""
  kafka:
    addresses: [] # No default (required)
    topics: [] # No default (required)
    target_version: 2.1.0 # No default (optional)
    consumer_group: "" # No default (optional)
    checkpoint_limit: 1024
    auto_replay_nacks: true
```

--
Advanced::
+
--

```yml
# All configuration fields, showing default values
input:
  label:
  kafka:
    addresses: [] # No default (required)
    topics: [] # No default (required)
    target_version: 2.1.0 # No default (optional)
    tls:
      enabled: false
      skip_cert_verify: false
      enable_renegotiation: false
      root_cas: "" # No default (optional)
      root_cas_file: "" # No default (optional)
      client_certs: [] # No default (optional)
    sasl:
      mechanism: none
      user: "" # No default (optional)
      password: "" # No default (optional)
      access_token: "" # No default (optional)
      token_cache: "" # No default (optional)
      token_key: "" # No default (optional)
    consumer_group: "" # No default (optional)
    client_id: benthos
    rack_id: "" # No default (optional)
    instance_id: "" # No default (optional) 
    start_from_oldest: true
    checkpoint_limit: 1024
    auto_replay_nacks: true
    commit_period: 1s
    max_processing_period: 100ms
    extract_tracing_map: root = @ # No default (optional)
    group:
      session_timeout: 10s
      heartbeat_interval: 3s
      rebalance_timeout: 60s
    fetch_buffer_cap: 256
    multi_header: false
    batching:
      count: 0
      byte_size: 0
      period: "" # No default (optional)
      check: "" # No default (optional)
      processors: [] # No default (optional)
```

--
======

Offsets are managed within Kafka under the specified consumer group, and partitions for each topic are automatically balanced across members of the consumer group.

The Kafka input allows parallel processing of messages from different topic partitions, and messages of the same topic partition are processed with a maximum parallelism determined by the field <<checkpoint_limit,`checkpoint_limit`>>.

To enforce ordered processing of partition messages, set the <<checkpoint_limit,`checkpoint_limit`>> to `1`, which makes sure that a message is only processed after the previous message is delivered.

Batching messages before processing can be enabled using the <<batching,`batching`>> field, and this batching is performed per-partition such that messages of a batch will always originate from the same partition. This batching mechanism is capable of creating batches of greater size than the <<checkpoint_limit,`checkpoint_limit`>>, in which case the next batch will only be created upon delivery of the current one.

== Metadata

This input adds the following metadata fields to each message:

- kafka_key
- kafka_topic
- kafka_partition
- kafka_offset
- kafka_lag
- kafka_timestamp_ms
- kafka_timestamp_unix
- kafka_tombstone_message
- All existing message headers (version 0.11+)

The field `kafka_lag` is the calculated difference between the high water mark offset of the partition at the time of ingestion and the current message offset.

You can access these metadata fields using xref:configuration:interpolation.adoc#bloblang-queries[function interpolation].

== Ordering

By default messages of a topic partition can be processed in parallel, up to a limit determined by the field `checkpoint_limit`. However, if strict ordered processing is required then this value must be set to 1 in order to process shard messages in lock-step. When doing so it is recommended that you perform batching at this component for performance as it will not be possible to batch lock-stepped messages at the output level.

== Troubleshooting

If you're seeing issues writing to or reading from Kafka with this component then it's worth trying out the newer xref:components:inputs/kafka_franz.adoc[`kafka_franz` input].

- I'm seeing logs that report `Failed to connect to kafka: kafka: client has run out of available brokers to talk to (Is your cluster reachable?)`, but the brokers are definitely reachable.

Unfortunately this error message will appear for a wide range of connection problems even when the broker endpoint can be reached. Double check your authentication configuration and also ensure that you have <<tlsenabled, enabled TLS>> if applicable.

include::redpanda-connect:components:partial$fields/inputs/kafka.adoc[]

// end::single-source[]