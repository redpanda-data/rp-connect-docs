= aws_s3
// tag::single-source[]
:page-commercial-names: Amazon S3, AWS S3, S3, Simple Storage Service
:type: output
:status: stable
:categories: ["Services","AWS"]

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Sends message parts as objects to an Amazon S3 bucket. Each object is uploaded with the path specified with the `path` field.

ifndef::env-cloud[]
Introduced in version 3.36.0.
endif::[]

[tabs]
======
Common::
+
--

```yml
include::components:example$common/outputs/aws_s3.yaml[]
```

--
Advanced::
+
--

```yml
include::components:example$advanced/outputs/aws_s3.yaml[]
```

--
======

In order to have a different path for each object you should use function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries], which are calculated per message of a batch.

== Metadata

Metadata fields on messages will be sent as headers, in order to mutate these values (or remove them) check out the xref:configuration:metadata.adoc[metadata docs].

== Tags

The tags field allows you to specify key/value pairs to attach to objects as tags, where the values support xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions]:

```yaml
output:
  aws_s3:
    bucket: TODO
    path: ${!counter()}-${!timestamp_unix_nano()}.tar.gz
    tags:
      Key1: Value1
      Timestamp: ${!meta("Timestamp")}
```

=== Credentials

By default Redpanda Connect will use a shared credentials file when connecting to AWS services. It's also possible to set them explicitly at the component level, allowing you to transfer data across accounts. You can find out more in xref:guides:cloud/aws.adoc[].

== Batching

It's common to want to upload messages to S3 as batched archives. The easiest way to do this is to batch your messages at the output level and join the batch of messages with an xref:components:processors/archive.adoc[`archive`] or xref:components:processors/compress.adoc[`compress`] processor.

For example, the following configuration uploads messages as a .tar.gz archive of documents: 

```yaml
output:
  aws_s3:
    bucket: TODO
    path: ${!counter()}-${!timestamp_unix_nano()}.tar.gz
    batching:
      count: 100
      period: 10s
      processors:
        - archive:
            format: tar
        - compress:
            algorithm: gzip
```

Alternatively, this configuration uploads JSON documents as a single large document containing an array of objects:

```yaml
output:
  aws_s3:
    bucket: TODO
    path: ${!counter()}-${!timestamp_unix_nano()}.json
    batching:
      count: 100
      processors:
        - archive:
            format: json_array
```

== Performance

This output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.

include::redpanda-connect:components:partial$fields/outputs/aws_s3.adoc[]

// end::single-source[]