= gcp_cloud_storage
// tag::single-source[]
:type: output
:status: beta
:categories: ["Services","GCP"]

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Sends message parts as objects to a Google Cloud Storage bucket. Each object is uploaded with the path specified with the `path` field.

ifndef::env-cloud[]
Introduced in version 3.43.0.
endif::[]

[tabs]
======
Common::
+
--

```yml
# Common configuration fields, showing default values
output:
  label: ""
  gcp_cloud_storage:
    bucket: "" # No default (required)
    path: ${!counter()}-${!timestamp_unix_nano()}.txt
    content_type: application/octet-stream
    collision_mode: overwrite
    timeout: 3s
    credentials_json: "" # No default (optional)
    max_in_flight: 64
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

--
Advanced::
+
--

```yml
# All configuration fields, showing default values
output:
  label: ""
  gcp_cloud_storage:
    bucket: "" # No default (required)
    path: ${!counter()}-${!timestamp_unix_nano()}.txt
    content_type: application/octet-stream
    content_encoding: ""
    collision_mode: overwrite
    chunk_size: 16777216
    timeout: 3s
    credentials_json: "" # No default (optional)
    max_in_flight: 64
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: [] # No default (optional)
```

--
======

In order to have a different path for each object you should use function interpolations described in xref:configuration:interpolation.adoc#bloblang-queries[Bloblang queries], which are calculated per message of a batch.

== Metadata

Metadata fields on messages will be sent as headers, in order to mutate these values (or remove them) check out the xref:configuration:metadata.adoc[metadata docs].

== Credentials

By default Redpanda Connect will use a shared credentials file when connecting to GCP services. You can find out more in xref:guides:cloud/gcp.adoc[].

== Batching

It's common to want to upload messages to Google Cloud Storage as batched archives, the easiest way to do this is to batch your messages at the output level and join the batch of messages with an xref:components:processors/archive.adoc[`archive`] and/or xref:components:processors/compress.adoc[`compress`] processor.

For example, if we wished to upload messages as a .tar.gz archive of documents we could achieve that with the following config:

```yaml
output:
  gcp_cloud_storage:
    bucket: TODO
    path: ${!counter()}-${!timestamp_unix_nano()}.tar.gz
    batching:
      count: 100
      period: 10s
      processors:
        - archive:
            format: tar
        - compress:
            algorithm: gzip
```

Alternatively, if we wished to upload JSON documents as a single large document containing an array of objects we can do that with:

```yaml
output:
  gcp_cloud_storage:
    bucket: TODO
    path: ${!counter()}-${!timestamp_unix_nano()}.json
    batching:
      count: 100
      processors:
        - archive:
            format: json_array
```

== Performance

This output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.

This output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc].

include::redpanda-connect:components:partial$fields/outputs/gcp_cloud_storage.adoc[]

// end::single-source[]