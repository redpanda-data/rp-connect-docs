= gcp_pubsub
// tag::single-source[]
:type: output
:status: stable
:categories: ["Services","GCP"]

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]


Sends messages to a GCP Cloud Pub/Sub topic. xref:configuration:metadata.adoc[Metadata] from messages are sent as attributes.


[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
output:
  label: ""
  gcp_pubsub:
    project: "" # No default (required)
    credentials_json: "" # No default (optional)
    topic: "" # No default (required)
    endpoint: ""
    max_in_flight: 64
    count_threshold: 100
    delay_threshold: 10ms
    byte_threshold: 1000000
    metadata:
      exclude_prefixes: []
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
output:
  label: ""
  gcp_pubsub:
    project: "" # No default (required)
    credentials_json: "" # No default (optional)
    topic: "" # No default (required)
    endpoint: ""
    ordering_key: "" # No default (optional)
    max_in_flight: 64
    count_threshold: 100
    delay_threshold: 10ms
    byte_threshold: 1000000
    publish_timeout: 1m0s
    validate_topic: true
    metadata:
      exclude_prefixes: []
    flow_control:
      max_outstanding_bytes: -1
      max_outstanding_messages: 1000
      limit_exceeded_behavior: block
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: [] # No default (optional)
```

--
======

For information on how to set up credentials, see https://cloud.google.com/docs/authentication/production[this guide^].

== Troubleshooting

If you're consistently seeing `Failed to send message to gcp_pubsub: context deadline exceeded` error logs without any further information it is possible that you are encountering https://github.com/benthosdev/benthos/issues/1042, which occurs when metadata values contain characters that are not valid utf-8. This can frequently occur when consuming from Kafka as the key metadata field may be populated with an arbitrary binary value, but this issue is not exclusive to Kafka.

If you are blocked by this issue then a work around is to delete either the specific problematic keys:

```yaml
pipeline:
  processors:
    - mapping: |
        meta kafka_key = deleted()
```

Or delete all keys with:

```yaml
pipeline:
  processors:
    - mapping: meta = deleted()
```

include::redpanda-connect:components:partial$fields/outputs/gcp_pubsub.adoc[]

// end::single-source[]