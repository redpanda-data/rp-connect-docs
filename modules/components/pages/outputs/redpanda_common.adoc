= redpanda_common
// tag::single-source[]
:type: output
:categories: ["Services"]

component_type_dropdown::[]


Sends data to a Redpanda (Kafka) broker, using credentials from a common `redpanda` configuration block.

To avoid duplicating Redpanda cluster credentials in your `redpanda_common` input, output, or any other components in your data pipeline, you can use a single xref:components:redpanda/about.adoc[`redpanda` configuration block]. For more details, see the <<pipeline-example,Pipeline example>>.

ifndef::env-cloud[]
Introduced in version 4.39.0.
endif::[]

NOTE: If you need to move topic data between Redpanda clusters or other Apache Kafka clusters, consider using the xref:components:inputs/redpanda.adoc[`redpanda` input] and xref:components:outputs/redpanda.adoc[output] instead.

[tabs]
======
Common::
+
--

```yml
# Common configuration fields, showing default values
output:
  label: ""
  redpanda_common:
    topic: "" # No default (required)
    key: "" # No default (optional)
    partition: ${! meta("partition") } # No default (optional)
    metadata:
      include_prefixes: []
      include_patterns: []
    max_in_flight: 10
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

--
Advanced::
+
--

```yml
# All configuration fields, showing default values
output:
  label: ""
  redpanda_common:
    topic: "" # No default (required)
    key: "" # No default (optional)
    partition: ${! meta("partition") } # No default (optional)
    metadata:
      include_prefixes: []
      include_patterns: []
    timestamp_ms: ${! timestamp_unix_milli() } # No default (optional)
    max_in_flight: 10
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: [] # No default (optional)
```

--
======

== Pipeline example

This data pipeline reads data from `topic_A` and `topic_B` on a Redpanda cluster, and then writes the data to `topic_C` on the same cluster. The cluster details are configured within the `redpanda` configuration block, so you only need to configure them once. This is a useful feature when you have multiple inputs and outputs in the same data pipeline that need to connect to the same cluster.

```
input:
  redpanda_common:
    topics: [ topic_A, topic_B ]

output:
  redpanda_common:
    topic: topic_C
    key: ${! @id }

redpanda:
  seed_brokers: [ "127.0.0.1:9092" ]
  tls:
    enabled: true
  sasl:
    - mechanism: SCRAM-SHA-512
      password: bar
      username: foo

```

include::redpanda-connect:components:partial$fields/outputs/redpanda_common.adoc[]

// end::single-source[]