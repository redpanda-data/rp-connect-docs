= snowflake_streaming
// tag::single-source[]
:type: output
:categories: ["Services"]

component_type_dropdown::[]

Allows Snowflake to ingest data from your data pipeline using https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview[Snowpipe Streaming^].

To help you configure your own `snowflake_streaming` output, this page includes <<example-pipelines,example data pipelines>>.

ifndef::env-cloud[]
Introduced in version 4.39.0.
endif::[]

[tabs]
======
Common::
+
--

```yml
# Common configuration fields, showing default values
output:
  label: ""
  snowflake_streaming:
    account: ORG-ACCOUNT # No default (required)
    user: "" # No default (required)
    role: ACCOUNTADMIN # No default (required)
    database: MYDATABASE # No default (required)
    schema: PUBLIC # No default (required)
    table: MYTABLE # No default (required)
    private_key: "" # No default (optional)
    private_key_file: "" # No default (optional)
    private_key_pass: "" # No default (optional)
    mapping: "" # No default (optional)
    init_statement: | # No default (optional)
      CREATE TABLE IF NOT EXISTS mytable (amount NUMBER);
    schema_evolution:
      enabled: false # No default (required)
      processors: [] # No default (optional)
    batching:
      count: 0
      byte_size: 0
      period: "" # No default (optional)
      check: "" # No default (optional)
    max_in_flight: 4
```

--
Advanced::
+
--

```yml
# All configuration fields, showing default values
output:
  label: ""
  snowflake_streaming:
    account: ORG-ACCOUNT # No default (required)
    url: https://org-account.privatelink.snowflakecomputing.com # No default (optional)
    user: "" # No default (required)
    role: ACCOUNTADMIN # No default (required)
    database: MYDATABASE # No default (required)
    schema: PUBLIC # No default (required)
    table: MYTABLE # No default (required)
    private_key: "" # No default (optional)
    private_key_file: "" # No default (optional)
    private_key_pass: "" # No default (optional)
    mapping: "" # No default (optional)
    init_statement: | # No default (optional)
      CREATE TABLE IF NOT EXISTS mytable (amount NUMBER);
    schema_evolution:
      enabled: false # No default (required)
      ignore_nulls: true
      processors: [] # No default (optional)
    build_options:
      parallelism: 1
      chunk_size: 50000
    batching:
      count: 0
      byte_size: 0
      period: "" # No default (optional)
      check: "" # No default (optional)
      processors: [] # No default (optional)
    max_in_flight: 4
    channel_prefix: channel-${HOST} # No default (optional)
    channel_name: partition-${!@kafka_partition} # No default (optional)
    offset_token: offset-${!"%016X".format(@kafka_offset)} # No default (optional)
    commit_timeout: 60s
```

--
======

== Conversion of message data into Snowflake table rows

Message data conversion to Snowflake table rows is determined by the: 

- Output message contents.
- <<schema_evolution, Schema evolution settings>>.
- Schema of the <<table, target Snowflake table>>. 

The following scenarios highlight how these three factors affect data written to the target table.

NOTE: For reduced complexity, consider <<schema_evolution, turning on schema evolution>>, which automatically creates and updates the Snowflake table schema based on message contents.

=== Scenario: Data and table schema match (schema evolution turned on or off) 

An output message matches the existing table schema, and the `schema_evolution.enabled` field is set to `true` or `false`.

The target Snowflake table has two columns:

- `product_id` (NUMBER)
- `product_code` (STRING)

A pipeline generates the following message:

```json
{"product_id": 521, "product_code": “EST-PR”}
```

In this scenario:

- The JSON keys in the message (`"product_id"` and `"product_code"`) match column names in the target Snowflake table.
- The message values match the column data types. (If there was a data mismatch, the message would be rejected.)
- Redpanda Connect inserts the message values into a new row in the target Snowflake table.
+
|===
| product_id | product_code

^| 521
^| EST-PR
|===
 
=== Scenario: Data and table schema mismatch (schema evolution turned on) 

An output message includes schema updates, and the `schema_evolution.enabled` field is set to `true`.

The target Snowflake table has the same two columns as the <<scenario-data-and-table-schema-match-schema-evolution-turned-on-or-off, previous scenario>>:

- `product_id` (NUMBER)
- `product_code` (STRING)

This time, the pipeline generates the following message:

```json
{"product_batch": 11111, "product_color": “yellow”}
```

In this scenario:

- The JSON keys (`"product_batch"` and `"product_color"`) do not match column names in the target Snowflake table.
- As schema evolution is enabled, Redpanda Connect adds two new columns to the target table with data types derived from the output message values. For more information about the mapping of data types, see <<supported-data-formats-for-snowflake-columns, Supported data formats for Snowflake columns>>.
- Redpanda Connect inserts the message values into a new table row.
+
|===
| product_id | product_code | product_batch | product_color

^| (null)
^| (null)
^| 11111
^| yellow

|===
+
NOTE: You can <<schema_evolution-processors,configure processors>> to override the schema updates derived from the message values.

=== Scenario: Data and table schema mismatch (schema evolution turned off)

An output message includes schema updates, and the `schema_evolution.enabled` field is set to `false`.

The target Snowflake table has the same two columns:

- `product_id` (NUMBER)
- `product_code` (STRING)

The pipeline generates the same message as the <<scenario-data-and-table-schema-mismatch-schema-evolution-turned-on,previous scenario>>:

```json
{"product_batch": 11111, "product_color": “yellow”}
```

In this scenario:

- The JSON keys (`"product_batch"` and `"product_color"`) do not match any existing column names.
- Because schema evolution is turned off, Redpanda Connect ignores the extra column names and values and inserts a row of null values.
+
|===
| product_id | product_code

^| (null)
^| (null)

|===

== Supported data formats for Snowflake columns

The message data from your output must match the columns in the Snowflake table that you want to write data to. The following table shows you the https://docs.snowflake.com/en/sql-reference/intro-summary-data-types[column data types supported by Snowflake^] and how they correspond to the xref:guides:bloblang/methods.adoc#type[Bloblang data types] in Redpanda Connect.

|===
| Snowflake column data type | Bloblang data types

| CHAR, VARCHAR
| `string`

| BINARY
| `string` or `bytes`

| NUMBER
| `number`, or `string` where the `string` is parsed into a `number`

| FLOAT, including special values, such as `NaN` (Not a Number), `-inf` (negative infinity), and `inf` (positive infinity)
| `number`

| BOOLEAN
| `bool`, or `number` where a non-zero number is `true`

| TIME, DATE, TIMESTAMP 
| `timestamp`, or `number` where the `number` is a converted to a Unix timestamp, or `string` where the `string` is parsed using RFC 3339 format

| VARIANT, ARRAY, OBJECT
| Any data type converted into JSON

| GEOGRAPHY,GEOMETRY
| Not supported

|===


== Authentication

You can authenticate with Snowflake using an https://docs.snowflake.com/en/user-guide/key-pair-auth[RSA key pair^]. Either specify:

* A PEM-encoded private key, in the <<private_key,`private_key` field>>.
* The path to a file from which the output can load the private RSA key, in the <<private_key_file,`private_key_file` field>>.


== Performance

For improved performance, this output:

* Sends multiple messages in parallel. You can tune the maximum number of in-flight messages (or message batches) with the field `max_in_flight`.
* Sends messages as a batch. You can configure batches at both the input and output level. For more information, see xref:configuration:batching.adoc[Message Batching].

=== Batch sizes

Redpanda recommends that every message batch writes at least 16 MiB of compressed output to Snowflake. You can monitor batch sizes using the `snowflake_compressed_output_size_bytes` metric.

=== Metrics

This output emits the following metrics.

|===
| Metric name | Description

| `snowflake_compressed_output_size_bytes`
| The size in bytes of each message batch uploaded to Snowflake.

| `snowflake_convert_latency_ns`
| The time taken to convert messages into the Snowflake column data types.

| `snowflake_serialize_latency_ns`
| The time taken to serialize the converted columnar data into a file for upload to Snowflake.

| `snowflake_build_output_latency_ns`
| The time taken to build the file that is uploaded to Snowflake. This metric is the sum of `snowflake_convert_latency_ns` + `snowflake_serialize_latency_ns`. 

| `snowflake_upload_latency_ns`
| The time taken to upload the output file to Snowflake.

| `snowflake_register_latency_ns`
| The time taken to register the uploaded output file with Snowflake.

| `snowflake_commit_latency_ns`
| The time taken to commit the uploaded data updates to the target Snowflake table.

|===

include::redpanda-connect:components:partial$fields/outputs/snowflake_streaming.adoc[]

== Example pipelines

The following examples show you how to ingest, process, and write data to Snowflake from:

* A PostgreSQL table using change data capture (CDC)
* A Redpanda cluster
* A REST API that posts JSON payloads to a HTTP server

ifndef::env-cloud[]
See also: xref:cookbooks:snowflake_ingestion.adoc[Ingest data into Snowflake cookbook]
endif::[]

ifdef::env-cloud[]
See also: xref:develop:connect/cookbooks/snowflake_ingestion.adoc[Ingest data into Snowflake cookbook]
endif::[]

[tabs]
======
Write data exactly once to a Snowflake table using CDC::
+
--
Send data from a PostgreSQL table and write it to Snowflake exactly once using PostgreSQL logical replication.

This example includes some important features:

* To make sure that a Snowflake streaming channel does not assume that older data is already committed, the configuration sets a 45-second interval between message batches. This interval prevents a message batch from being sent while another batch is retried.
* The log sequence number of each data update from the Write-Ahead Log (WAL) in PostgreSQL makes sure that data is only uploaded once to the `snowflake_streaming` output, and that messages sent to the output are already lexicographically ordered.

NOTE: To do exactly-once data delivery, it's important that records are delivered in order to the output, and are correctly partitioned. Before you start, read the <<offset_token,`offset_token`>> field description. Alternatively, remove the `offset_token` field to use Redpanda Connect's default at-least-once delivery model.

```yaml
input:
  postgres_cdc:
    dsn: postgres://foouser:foopass@localhost:5432/foodb
    schema: "public"
    tables: ["my_pg_table"]
    # Use very large batches. Each batch is sent to Snowflake individually,
    # so to optimize query performance, use the largest file size
    # your memory allows
    batching:
      count: 50000
      period: 45s
    # Set an interval between message batches to prevent multiple batches 
    # from being in flight at once
    checkpoint_limit: 1
output:
  snowflake_streaming:
    # Using the log sequence number makes sure data is only updated exactly once
    offset_token: "${!@lsn}"
    # Sending a single ordered log means you can only send one update
    # at a time and properly increment the offset_token
    # and use only a single channel.
    max_in_flight: 1
    account: "MYSNOW-ACCOUNT"
    user: MYUSER
    role: ACCOUNTADMIN
    database: "MYDATABASE"
    schema: "PUBLIC"
    table: "MY_PG_TABLE"
    private_key_file: "my/private/key.p8"
```
--
Ingest data exactly once from Redpanda::
+
--
Ingest data from Redpanda using consumer groups, decode the schema using the schema registry, then write the corresponding data into Snowflake.

This example includes some important features:

* To create multiple Redpanda Connect streams to write to each output table, you need a unique channel prefix per stream. The `channel_prefix` field constructs a unique prefix for each stream using the host name.
ifndef::env-cloud[]
* To prevent message failures from being retried and changing the order of delivered messages, a dead-letter queue processes them. For more details about message ordering, see the xref:components:inputs/redpanda_common.adoc#ordering[`redpanda_common` input] documentation.
endif::[]
ifdef::env-cloud[]
* To prevent message failures from being retried and changing the order of delivered messages, a dead-letter queue processes them.
endif::[]

NOTE: To do exactly-once data delivery, it's important that records are delivered in order to the output, and are correctly partitioned. Before you start, read the <<channel_name, `channel_name`>> and <<offset_token, `offset_token`>> field descriptions. Alternatively, remove the `offset_token` field to use Redpanda Connect's default at-least-once delivery model.

```yaml
input:
  redpanda_common:
    topics: ["my_topic_going_to_snow"]
    consumer_group: "redpanda_connect_to_snowflake"
    # Use very large batches. Each batch is sent to Snowflake individually,
    # so to optimize query performance, use the largest file size 
    # your memory allows
    fetch_max_bytes: 100MiB
    fetch_min_bytes: 50MiB
    partition_buffer_bytes: 100MiB
pipeline:
  processors:
    - schema_registry_decode:
        url: "redpanda.example.com:8081"
        basic_auth:
          enabled: true
          username: MY_USER_NAME
          password: "${TODO}"
output:
  fallback:
    - snowflake_streaming:
        # To write an ordered stream of messages, each partition in 
        # Apache Kafka gets its own channel.
        channel_name: "partition-${!@kafka_partition}"
        # Offsets are lexicographically sorted in string form by padding with
        # leading zeros
        offset_token: offset-${!"%016X".format(@kafka_offset)}
        account: "MYSNOW-ACCOUNT"
        user: MYUSER
        role: ACCOUNTADMIN
        database: "MYDATABASE"
        schema: "PUBLIC"
        table: "MYTABLE"
        private_key_file: "my/private/key.p8"
        schema_evolution:
          enabled: true
    # To prevent delivery failures from changing the order of 
    # delivered records, it's important that they are immediately 
    # sent to a dead-letter queue.
    - retry:
        output:
          redpanda_common:
            topic: "dead_letter_queue"
```
--
HTTP server to push data to Snowflake::
+
--
Create a HTTP server input that receives HTTP PUT requests with JSON payloads. 

The payloads are buffered locally then written to Snowflake in batches. To create multiple Redpanda Connect streams to write to each output table, you need a unique channel prefix per stream. In this example, the `channel_prefix` field constructs a unique prefix for each stream using the host name.

NOTE: Using a buffer to immediately respond to the HTTP requests may result in data loss if there are delivery failures between the output and Snowflake.

For more information about the configuration of buffers, see xref:components:buffers/memory.adoc[buffers]. Alternatively, remove the buffer entirely to respond to the HTTP request only once the data is written to Snowflake.

```yaml
input:
  http_server:
    path: /snowflake
buffer:
  memory:
    # Max inflight data before applying backpressure
    limit: 524288000 # 50MiB
    # Batching policy the size of the files sent to Snowflake
    batch_policy:
      enabled: true
      byte_size: 33554432 # 32MiB
      period: "10s"
output:
  snowflake_streaming:
    account: "MYSNOW-ACCOUNT"
    user: MYUSER
    role: ACCOUNTADMIN
    database: "MYDATABASE"
    schema: "PUBLIC"
    table: "MYTABLE"
    private_key_file: "my/private/key.p8"
    channel_prefix: "snowflake-channel-for-${HOST}"
    schema_evolution:
      enabled: true
```
--
======

// end::single-source[]
