= ollama_chat
// tag::single-source[]
:type: processor
:categories: ["AI"]

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]

include::components:partial$ollama_warning.adoc[]


Generates responses to messages in a chat conversation using the Ollama API and external tools.

ifndef::env-cloud[]
Introduced in version 4.32.0.
endif::[]

[tabs]
======
Common::
+
--

```yml
include::components:example$common/processors/ollama_chat.yaml[]
```

--
Advanced::
+
--

```yml
include::components:example$advanced/processors/ollama_chat.yaml[]
```

--
======

This processor sends prompts to your chosen Ollama large language model (LLM) and generates text from the responses using the Ollama API and external tools.

By default, the processor starts and runs a locally-installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].

For more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^] and <<Examples, examples>>.

include::redpanda-connect:components:partial$fields/processors/ollama_chat.adoc[]

include::redpanda-connect:components:partial$examples/processors/ollama_chat.adoc[]

// end::single-source[]