= ollama_embeddings
// tag::single-source[]
:type: processor
:categories: ["AI"]

// Â© 2024 Redpanda Data Inc.


component_type_dropdown::[]

include::components:partial$ollama_warning.adoc[]

Generates vector embeddings from text, using the Ollama API.

ifndef::env-cloud[]
Introduced in version 4.32.0.
endif::[]

[tabs]
======
Common::
+
--

```yml
# Common config fields, showing default values
label: ""
ollama_embeddings:
  model: nomic-embed-text # No default (required)
  text: "" # No default (optional)
  runner:
    context_size: 0 # No default (optional)
    batch_size: 0 # No default (optional)
  server_address: http://127.0.0.1:11434 # No default (optional)
```

--
Advanced::
+
--

```yml
# All config fields, showing default values
label: ""
ollama_embeddings:
  model: nomic-embed-text # No default (required)
  text: "" # No default (optional)
  runner:
    context_size: 0 # No default (optional)
    batch_size: 0 # No default (optional)
    gpu_layers: 0 # No default (optional)
    threads: 0 # No default (optional)
    use_mmap: false # No default (optional)
    use_mlock: false # No default (optional)
  server_address: http://127.0.0.1:11434 # No default (optional)
  cache_directory: /opt/cache/connect/ollama # No default (optional)
  download_url: "" # No default (optional)
```

--
======

This processor sends text to your chosen Ollama large language model (LLM) and creates vector embeddings, using the Ollama API. Vector embeddings are long arrays of numbers that represent values or objects, in this case text. 

By default, the processor starts and runs a locally installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].

For more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^].

include::redpanda-connect:components:partial$fields/processors/ollama_embeddings.adoc[]

// end::single-source[]