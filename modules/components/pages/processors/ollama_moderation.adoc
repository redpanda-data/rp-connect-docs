= ollama_moderation
// tag::single-source[]
:type: processor
:categories: ["AI"]

component_type_dropdown::[]

include::components:partial$ollama_warning.adoc[]

Generates responses to messages in a chat conversation using the Ollama API, and checks the responses to make sure they do not violate https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/[safety or security standards^].

ifndef::env-cloud[]
Introduced in version 4.45.0.
endif::[]

[tabs]
======
Common::
+
--

```yml
# Common configuration fields, showing default values
label: ""
ollama_moderation:
  model: llama-guard3 # No default (required)
  prompt: "" # No default (required)
  response: "" # No default (required)
  runner:
    context_size: 0 # No default (optional)
    batch_size: 0 # No default (optional)
  server_address: http://127.0.0.1:11434 # No default (optional)
```

--
Advanced::
+
--

```yml
# All configuration fields, showing default values
label: ""
ollama_moderation:
  model: llama-guard3 # No default (required)
  prompt: "" # No default (required)
  response: "" # No default (required)
  runner:
    context_size: 0 # No default (optional)
    batch_size: 0 # No default (optional)
    gpu_layers: 0 # No default (optional)
    threads: 0 # No default (optional)
    use_mmap: false # No default (optional)
    use_mlock: false # No default (optional)
  server_address: http://127.0.0.1:11434 # No default (optional)
  cache_directory: /opt/cache/connect/ollama # No default (optional)
  download_url: "" # No default (optional)
```

--
======

This processor checks the safety of responses from your chosen large language model (LLM) using either https://ollama.com/library/llama-guard3[Llama Guard 3^] or https://ollama.com/library/shieldgemma[ShieldGemma^]. 

By default, the processor starts and runs a locally-installed Ollama server. Alternatively, to use an already running Ollama server, add your server details to the `server_address` field. You can https://ollama.com/download[download and install Ollama from the Ollama website^].

For more information, see the https://github.com/ollama/ollama/tree/main/docs[Ollama documentation^] and <<Examples, Examples>>.

To check the safety of your prompts, see the xref:components:processors/ollama_chat.adoc#examples[`ollama_chat` processor] documentation.

include::redpanda-connect:components:partial$fields/processors/ollama_moderation.adoc[]

include::redpanda-connect:components:partial$examples/processors/ollama_moderation.adoc[]

// end::single-source[]