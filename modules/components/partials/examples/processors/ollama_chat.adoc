// This content is autogenerated. Do not edit manually.

== Examples

=== Use Llava to analyze an image

This example fetches image URLs from stdin and has a multimodal LLM describe the image.

[source,yaml]
----
input:
  stdin:
    scanner:
      lines: {}
pipeline:
  processors:
    - http:
        verb: GET
        url: "${!content().string()}"
    - ollama_chat:
        model: llava
        prompt: "Describe the following image"
        image: "root = content()"
output:
  stdout:
    codec: lines
----

=== Use subpipelines as tool calls

This example allows llama3.2 to execute a subpipeline as a tool call to get more data.

[source,yaml]
----
input:
  generate:
    count: 1
    mapping: |
      root = "What is the weather like in Chicago?"
pipeline:
  processors:
    - ollama_chat:
        model: llama3.2
        prompt: "${!content().string()}"
        tools:
          - name: GetWeather
            description: "Retrieve the weather for a specific city"
            parameters:
              required: ["city"]
              properties:
                city:
                  type: string
                  description: the city to lookup the weather for
            processors:
              - http:
                  verb: GET
                  url: 'https://wttr.in/${!this.city}?T'
                  headers:
                    # Spoof curl user-ageent to get a plaintext text
                    User-Agent: curl/8.11.1
output:
  stdout: {}
----


