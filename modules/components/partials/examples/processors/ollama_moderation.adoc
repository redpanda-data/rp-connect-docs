// This content is autogenerated. Do not edit manually.

== Examples

=== Use Llama Guard 3 classify a LLM response

This example uses Llama Guard 3 to check if another model responded with a safe or unsafe content.

[source,yaml]
----
input:
  stdin:
    scanner:
      lines: {}
pipeline:
  processors:
    - ollama_chat:
        model: llava
        prompt: "${!content().string()}"
        save_prompt_metadata: true
    - ollama_moderation:
        model: llama-guard3
        prompt: "${!@prompt}"
        response: "${!content().string()}"
    - mapping: |
        root.response = content().string()
        root.is_safe = @safe
output:
  stdout:
    codec: lines
----


