// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1247543314/Generate+reference+docs+for+Redpanda+Connect

== Fields

=== `auto_replay_nacks`

Whether to automatically replay messages that are rejected (nacked) at the output level. If the cause of rejections is persistent, leaving this option enabled can result in back pressure.

Set `auto_replay_nacks` to `false` to delete rejected messages. Disabling auto replays can greatly improve memory efficiency of high throughput streams, as the original shape of the data is discarded immediately upon consumption and mutation.

*Type*: `bool`

*Default*: `true`

=== `batching`

Configure a xref:configuration:batching.adoc[batching policy].

*Type*: `object`

[source,yaml]
----
# Examples:
batching:
  byte_size: 5000
  count: 0
  period: 1s
batching:
  count: 10
  period: 1s
batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
----

=== `batching.byte_size`

The number of bytes at which the batch is flushed. Set to `0` to disable size-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.check`

A xref:guides:bloblang/about.adoc[Bloblang query] that returns a boolean value indicating whether a message should end a batch.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
check: this.type == "end_of_transaction"
----

=== `batching.count`

The number of messages after which the batch is flushed. Set to `0` to disable count-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.period`

The period of time after which an incomplete batch is flushed regardless of its size. This field accepts Go duration format strings such as `100ms`, `1s`, or `5s`.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
period: 1s
period: 1m
period: 500ms
----

=== `batching.processors[]`

For aggregating and archiving message batches, you can add a list of xref:components:processors/about.adoc[processors] to apply to a batch as it is flushed. All resulting messages are flushed as a single batch even when you configure processors to split the batch into smaller batches.

*Type*: `processor`

[source,yaml]
----
# Examples:
processors:
  - archive:
      format: concatenate

  - archive:
      format: lines

  - archive:
      format: json_array

----

=== `checkpoint_cache`

A xref:components:caches/about.adoc[cache resource] to store the current Log Sequence Number (LSN) position. This enables the connector to resume from the last processed position after restarts, preventing data loss and duplicate processing. The cache stores the highest LSN that has been successfully delivered downstream.

*Type*: `string`

=== `checkpoint_key`

The key used to store the LSN checkpoint position in the cache. If you have multiple `microsoft_sql_server_cdc` inputs sharing the same cache resource, provide unique keys to avoid conflicts. The default key works well for single-instance deployments.

*Type*: `string`

*Default*: `microsoft_sql_server_cdc`

=== `checkpoint_limit`

The maximum number of messages that can be processed concurrently before applying back pressure. Higher values enable better parallelization and batching but increase memory usage. Messages are processed in LSN order, and a given LSN is only acknowledged after all previous LSNs have been successfully delivered, ensuring at-least-once guarantees.

*Type*: `int`

*Default*: `1024`

=== `connection_string`

The connection string for the Microsoft SQL Server database. Use the format `sqlserver://username:password@host/instance?param1=value&param2=value`. For Windows Authentication, use `sqlserver://host/instance?trusted_connection=yes`. Include additional parameters like `TrustServerCertificate=true` for self-signed certificates or `encrypt=disable` to disable encryption.

*Type*: `string`

[source,yaml]
----
# Examples:
connection_string: sqlserver://username:password@host/instance?param1=value&param2=value
----

=== `exclude[]`

Regular expressions for tables to exclude from CDC streaming. Use this to filter out specific tables from the include patterns. Table names should follow the `schema.table` format. Exclude patterns are applied after include patterns, allowing you to include broad patterns while excluding specific tables.

*Type*: `array`

[source,yaml]
----
# Examples:
exclude:
  - dbo.privatetable

----

=== `include[]`

Regular expressions for tables to include in CDC streaming. Specify table names using the format `schema.table` (such as `dbo.orders`, `sales.customers`). Each pattern is treated as a regular expression, allowing wildcards and pattern matching. All specified tables must have CDC enabled in SQL Server.

*Type*: `array`

[source,yaml]
----
# Examples:
include:
  - dbo.products

----

=== `snapshot_max_batch_size`

The maximum number of rows to stream in a single batch during the initial snapshot phase. Larger batch sizes can improve throughput for initial data loads but may increase memory usage. This setting only applies when `stream_snapshot` is enabled.

*Type*: `int`

*Default*: `1000`

=== `stream_backoff_interval`

The time interval to wait between polling attempts when no new CDC data is available. For low-traffic tables, increasing this value reduces database load and network traffic. Use Go duration format like `5s`, `30s`, or `1m`. Shorter intervals provide lower latency for new changes but increase server load.

*Type*: `string`

*Default*: `5s`

[source,yaml]
----
# Examples:
stream_backoff_interval: 5s
stream_backoff_interval: 1m
----

=== `stream_snapshot`

Whether to stream a snapshot of all existing data before streaming CDC changes. When enabled, the connector first queries all existing table data, then switches to streaming incremental changes from the transaction log. Set to `false` to start streaming only new changes from the current LSN position.

*Type*: `bool`


