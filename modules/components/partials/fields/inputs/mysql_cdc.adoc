// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1247543314/Generate+reference+docs+for+Redpanda+Connect

== Fields

=== `auto_replay_nacks`

Whether to automatically replay rejected messages (negative acknowledgements) at the output level. If the cause of rejections is persistent, leaving this option enabled can result in back pressure.

Set `auto_replay_nacks` to `false` to delete rejected messages. Disabling auto replays can greatly improve memory efficiency of high throughput streams as the original shape of the data is discarded immediately upon consumption and mutation.

*Type*: `bool`

*Default*: `true`

=== `batching`


Allows you to configure a xref:configuration:batching.adoc[batching policy].

*Type*: `object`

[source,yaml]
----
# Examples:
batching:
  byte_size: 5000
  count: 0
  period: 1s
batching:
  count: 10
  period: 1s
batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
----

=== `batching.byte_size`

The number of bytes at which the batch is flushed. Set to `0` to disable size-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.check`

A xref:guides:bloblang/about.adoc[Bloblang query] that returns a boolean value indicating whether a message should end a batch.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
check: this.type == "end_of_transaction"
----

=== `batching.count`

The number of messages after which the batch is flushed. Set to `0` to disable count-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.period`

The period of time after which an incomplete batch is flushed regardless of its size.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
period: 1s
period: 1m
period: 500ms
----

=== `batching.processors[]`

For aggregating and archiving message batches, you can add a list of xref:components:processors/about.adoc[processors] to apply to a batch as it is flushed. All resulting messages are flushed as a single batch even when you configure processors to split the batch into smaller batches.

*Type*: `processor`

[source,yaml]
----
# Examples:
processors:
  - archive:
      format: concatenate

  - archive:
      format: lines

  - archive:
      format: json_array

----

=== `checkpoint_cache`

Specify a `cache` resource to store the binlog position of the most recent data update delivered to Redpanda Connect. After a restart, Redpanda Connect can continue processing changes from this last known position, avoiding the need to reprocess all table updates.

*Type*: `string`

=== `checkpoint_key`

The key identifier used to store the binlog position in <<checkpoint_cache,`checkpoint_cache`>>. If you have multiple `mysql_cdc` inputs sharing the same cache, you can provide an alternative key.

*Type*: `string`

*Default*: `mysql_binlog_position`

=== `checkpoint_limit`

The maximum number of messages that this input can process at a given time. Increasing this limit enables parallel processing, and batching at the output level. To preserve at-least-once guarantees, any given binlog position is not acknowledged until all messages under that offset are delivered.

*Type*: `int`

*Default*: `1024`

=== `dsn`

The data source name (DSN) of the MySQL database from which you want to stream updates. Use the format `user:password@tcp(localhost:3306)/database`.

*Type*: `string`

[source,yaml]
----
# Examples:
dsn: user:password@tcp(localhost:3306)/database
----

=== `flavor`

The type of MySQL database to connect to.

*Type*: `string`

*Default*: `mysql`

[cols="1m,2a"]
|===
|Option |Summary

|mariadb
|MariaDB flavored databases.

|mysql
|MySQL flavored databases.

|===

=== `snapshot_max_batch_size`

The maximum number of table rows to fetch in each batch when taking a snapshot. This option is only available when `stream_snapshot` is set to `true`.

*Type*: `int`

*Default*: `1000`

=== `stream_snapshot`

When set to `true`, this input streams a snapshot of all existing data in the source database before streaming data changes. To use this setting, all database tables that you want to replicate _must_ have a primary key.

*Type*: `bool`

=== `tables[]`

A list of the database table names to stream changes from. Specify each table name as a separate item.

*Type*: `array`

[source,yaml]
----
# Examples:
tables:
  - table1
  - table2

----


