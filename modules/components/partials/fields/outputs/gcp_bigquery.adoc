// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1247543314/Generate+reference+docs+for+Redpanda+Connect

== Fields

=== `auto_detect`

Whether this component automatically infers the options and schema for `CSV` and `NEWLINE_DELIMITED_JSON` sources. 

If this value is set to `false` and the destination table doesn't exist, the output throws an insertion error as it is unable to insert data. 

CAUTION: This field delegates schema detection to the GCP BigQuery service. For the `CSV` format, values like `no` may be treated as booleans.

*Type*: `bool`

*Default*: `false`

=== `batching`

Configure a xref:configuration:batching.adoc[batching policy].

*Type*: `object`

[source,yaml]
----
# Examples:
batching:
  byte_size: 5000
  count: 0
  period: 1s
batching:
  count: 10
  period: 1s
batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
----

=== `batching.byte_size`

The amount of bytes at which the batch is flushed. Set to `0` to disable size-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.check`

A xref:guides:bloblang/about.adoc[Bloblang query] that returns a boolean value indicating whether a message should end a batch.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
check: this.type == "end_of_transaction"
----

=== `batching.count`

The number of messages after which the batch is flushed. Set to `0` to disable count-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.period`

The period of time after which an incomplete batch is flushed regardless of its size.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
period: 1s
period: 1m
period: 500ms
----

=== `batching.processors[]`

For aggregating and archiving message batches, you can add a list of xref:components:processors/about.adoc[processors] to apply to a batch as it is flushed. All resulting messages are flushed as a single batch even when you configure processors to split the batch into smaller batches.

*Type*: `processor`

[source,yaml]
----
# Examples:
processors:
  - archive:
      format: concatenate

  - archive:
      format: lines

  - archive:
      format: json_array

----

=== `create_disposition`

Specifies the circumstances under which a destination table is created. 

* Use `CREATE_IF_NEEDED` to create the destination table if it does not already exist. Tables are created atomically on successful completion of a job. 
* Use `CREATE_NEVER` if the destination table must already exist.

*Type*: `string`

*Default*: `CREATE_IF_NEEDED`

*Options*: `CREATE_IF_NEEDED`, `CREATE_NEVER`

=== `credentials_json`

Sets the  https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account[Google Service Account Credentials JSON^] (optional).

include::redpanda-connect:components:partial$secret_warning.adoc[]

*Type*: `string`

*Default*: `""`

=== `csv`

Specify how CSV data is interpreted.

*Type*: `object`

=== `csv.allow_jagged_rows`

Set to `true` to treat optional missing trailing columns as nulls in CSV data.

*Type*: `bool`

*Default*: `false`

=== `csv.allow_quoted_newlines`

Whether quoted data sections containing new lines are allowed when reading CSV data.

*Type*: `bool`

*Default*: `false`

=== `csv.encoding`

The character encoding of CSV data.

*Type*: `string`

*Default*: `UTF-8`

*Options*: `UTF-8`, `ISO-8859-1`

=== `csv.field_delimiter`

The separator for fields in a CSV file. The output uses this value when reading or exporting data.

*Type*: `string`

*Default*: `,`

=== `csv.header[]`

A list of values to use as the header for each batch of messages. If not specified, the first line of each message is used as the header.

*Type*: `array`

*Default*: `[]`

=== `csv.skip_leading_rows`

The number of rows at the top of a CSV file that BigQuery will skip when reading data. The default value is `1`, which allows Redpanda Connect to add the specified header in the first line of each batch sent to BigQuery.

*Type*: `int`

*Default*: `1`

=== `dataset`

The BigQuery Dataset ID.

*Type*: `string`

=== `format`

The format of each incoming message.

*Type*: `string`

*Default*: `NEWLINE_DELIMITED_JSON`

*Options*: `NEWLINE_DELIMITED_JSON`, `CSV`, `PARQUET`

=== `ignore_unknown_values`

Set this value to `true` to ignore values that do not match the schema:

* For the `CSV` format, extra values at the end of a line are ignored. 
* For the `NEWLINE_DELIMITED_JSON` format, values that do not match any column name are ignored. 

By default, this value is set to `false`, and records containing unknown values are treated as bad records. Use the `max_bad_records` field to customize how bad records are handled.

*Type*: `bool`

*Default*: `false`

=== `job_labels`

A list of labels to add to the load job.

*Type*: `string`

*Default*: `{}`

=== `job_project`

Specify the project ID in which jobs are executed. If not set, the `project` value is used.

*Type*: `string`

*Default*: `""`

=== `max_bad_records`

The maximum number of bad records to ignore when reading data and <<ignore_unknown_values, `ignore_unknown_values`>> is set to `true`.

*Type*: `int`

*Default*: `0`

=== `max_in_flight`

The maximum number of message batches to have in flight at a given time. Increase this value to improve throughput.

*Type*: `int`

*Default*: `64`

=== `project`

Specify the project ID of the dataset to insert data into. If not set, the project ID is inferred from the project linked to the service account or read from the `GOOGLE_CLOUD_PROJECT` environment variable.

*Type*: `string`

*Default*: `""`

=== `table`

The table to insert messages into.

*Type*: `string`

=== `write_disposition`

Specifies how existing data in a destination table is treated.

*Type*: `string`

*Default*: `WRITE_APPEND`

*Options*: `WRITE_APPEND`, `WRITE_EMPTY`, `WRITE_TRUNCATE`


