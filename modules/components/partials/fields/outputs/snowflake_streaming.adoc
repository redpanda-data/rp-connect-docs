// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1247543314/Generate+reference+docs+for+Redpanda+Connect

== Fields

=== `account`

The https://docs.snowflake.com/en/user-guide/admin-account-identifier#account-name[Snowflake account name to use^]. 

Use the format `<orgname>-<account_name>` where:

- The `<orgname>` is the name of your Snowflake organization.
- The `<account_name>` is the unique name of your account with your Snowflake organization.

To find the correct value for this field, run the following query in Snowflake:

```sql
WITH HOSTLIST AS 
(SELECT * FROM TABLE(FLATTEN(INPUT => PARSE_JSON(SYSTEM$allowlist()))))
SELECT REPLACE(VALUE:host,'.snowflakecomputing.com','') AS ACCOUNT_IDENTIFIER
FROM HOSTLIST
WHERE VALUE:type = 'SNOWFLAKE_DEPLOYMENT_REGIONLESS';
```

*Type*: `string`

[source,yaml]
----
# Examples:
account: ORG-ACCOUNT
----

=== `batching`

Lets you configure a xref:configuration:batching.adoc[batching policy].

Type*: `object`
```yml
# Examples
batching:
  byte_size: 5000
  count: 0
  period: 1s
batching:
  count: 10
  period: 1s
batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
```

*Type*: `object`

[source,yaml]
----
# Examples:
batching:
  byte_size: 5000
  count: 0
  period: 1s

# ---

batching:
  count: 10
  period: 1s

# ---

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
----

=== `batching.byte_size`

The number of bytes at which the batch is flushed. Set to `0` to disable size-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.check`

A xref:guides:bloblang/about.adoc[Bloblang query] that should return a boolean value indicating whether a message should end a batch.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
check: this.type == "end_of_transaction"
----

=== `batching.count`

The number of messages after which the batch is flushed. Set to `0` to disable count-based batching.

*Type*: `int`

*Default*: `0`

=== `batching.period`

The period of time after which an incomplete batch is flushed regardless of its size. This field accepts Go duration format strings such as `100ms`, `1s`, or `5s`.

*Type*: `string`

*Default*: `""`

[source,yaml]
----
# Examples:
period: 1s

# ---

period: 1m

# ---

period: 500ms
----

=== `batching.processors[]`

A list of xref:components:processors/about.adoc[processors] to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. All resulting messages are flushed as a single batch, and therefore splitting the batch into smaller batches using these processors is a no-op.

*Type*: `processor`

[source,yaml]
----
# Examples:
processors:
  - archive:
      format: concatenate


# ---

processors:
  - archive:
      format: lines


# ---

processors:
  - archive:
      format: json_array

----

=== `build_options`

Options for optimizing the build of the output data that is sent to Snowflake. Monitor the `snowflake_build_output_latency_ns` metric to assess whether you need to update these options.

*Type*: `object`

=== `build_options.chunk_size`

The number of table rows to submit in each chunk for processing.

*Type*: `int`

*Default*: `50000`

=== `build_options.parallelism`

The maximum amount of parallel processing to use when building the output for Snowflake.

*Type*: `int`

*Default*: `1`

=== `channel_name`

The channel name to use when connecting to a Snowflake table. Duplicate channel names cause errors and prevent multiple instances of Redpanda Connect from writing at the same time, and so this field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].

Redpanda Connect assumes that a message batch contains messages for a single channel, which means that interpolation is only executed on the first message in each batch. If your pipeline uses an input that is partitioned, such as an Apache Kafka topic, batch messages at the input level to make sure all messages are processed by the same channel.

You can specify either the `channel_name` or `channel_prefix`, but not both. If neither field is populated, this output creates a channel name based on a table's fully-qualified name, which results in a single stream per table.

NOTE: Snowflake limits the number of streams per table to 10,000. If you need to use more than 10,000 streams, contact https://www.snowflake.com/en/support/[Snowflake support^].

*Type*: `string`

[source,yaml]
----
# Examples:
channel_name: partition-${!@kafka_partition}
----

=== `channel_prefix`

The prefix to use when creating a channel name for connecting to a Snowflake table. Adding a `channel_prefix` avoids the creation of duplicate channel names, which result in errors and prevent multiple instances of Redpanda Connect from writing at the same time.

You can specify either the `channel_prefix` or `channel_name`, but not both. If neither field is populated, this output creates a channel name based on a table's fully-qualified name, which results in a single stream per table. 

The maximum number of channels open at any time is determined by the value in the `max_in_flight` field. 

NOTE: Snowflake limits the number of streams per table to 10,000. If you need to use more than 10,000 streams, contact https://www.snowflake.com/en/support/[Snowflake support^].

*Type*: `string`

[source,yaml]
----
# Examples:
channel_prefix: channel-${HOST}
----

=== `commit_timeout`

The maximum duration to wait while data updates from a message batch are asynchronously committed to Snowflake.

*Type*: `string`

*Default*: `60s`

[source,yaml]
----
# Examples:
commit_timeout: 10s

# ---

commit_timeout: 10m
----

=== `database`

The Snowflake database you want to write data to.

*Type*: `string`

[source,yaml]
----
# Examples:
database: MY_DATABASE
----

=== `init_statement`

Optional SQL statements to execute immediately after this output connects to Snowflake for the first time. This is a useful way to initialize tables before processing data. 

NOTE: Make sure your SQL statements are idempotent, so they do not cause issues when run multiple times after service restarts.

*Type*: `string`

[source,yaml]
----
# Examples:
init_statement: |-
  
  CREATE TABLE IF NOT EXISTS mytable (amount NUMBER);
  

# ---

init_statement: |-
  
  ALTER TABLE t1 ALTER COLUMN c1 DROP NOT NULL;
  ALTER TABLE t1 ADD COLUMN a2 NUMBER;
  
----

=== `mapping`

The xref:guides:bloblang/about.adoc[Bloblang `mapping`] to execute on each message.

*Type*: `string`

=== `max_in_flight`

The maximum number of messages to have in flight at a given time. Increase this number to improve throughput until performance plateaus.

*Type*: `int`

*Default*: `4`

=== `offset_token`

The offset token to use for exactly-once delivery of data to a Snowflake table. This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].

This output assumes that messages within a batch are in increasing order by offset token. When data is sent on a channel, the offset token of each message in the batch is compared to the latest token processed by the channel. If the offset token is lexicographically less than the latest token, it's assumed the message is a duplicate and is dropped. Messages must be delivered to the output in order, otherwise they are processed as duplicates and dropped.

To avoid dropping retried messages if later messages have succeeded in the meantime, use a dead-letter queue to process failed messages. See the <<example-pipelines, Ingesting data exactly once from Redpanda>> example.

NOTE: If you're using a numeric value as an offset token, pad the value so that it's lexicographically ordered in its string representation because offset tokens are compared in string form. For more details, see the <<example-pipelines, Ingesting data exactly once from Redpanda>> example.

For more information about offset tokens, see https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview#offset-tokens[Snowflake Documentation^].

*Type*: `string`

[source,yaml]
----
# Examples:
offset_token: offset-${!"%016X".format(@kafka_offset)}

# ---

offset_token: postgres-${!@lsn}
----

=== `private_key`

The PEM-encoded private RSA key to use for authentication with Snowflake. You must specify a value for this field or the `private_key_file` field.

include::redpanda-connect:components:partial$secret_warning.adoc[]

*Type*: `string`

=== `private_key_file`

A `.p8`, PEM-encoded file to load the private RSA key from. You must specify a value for this field or the `private_key` field.

*Type*: `string`

=== `private_key_pass`

If the RSA key is encrypted, specify the RSA key passphrase.

include::redpanda-connect:components:partial$secret_warning.adoc[]

*Type*: `string`

=== `role`

The role of the user specified in the `user` field. The user's role must have the https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview#required-access-privileges[required privileges^] to call the Snowpipe Streaming APIs. For more information about user roles, see the https://docs.snowflake.com/en/user-guide/admin-user-management#user-roles[Snowflake documentation^].

*Type*: `string`

[source,yaml]
----
# Examples:
role: ACCOUNTADMIN
----

=== `schema`

The schema of the Snowflake database you want to write data to.

*Type*: `string`

[source,yaml]
----
# Examples:
schema: PUBLIC
----

=== `schema_evolution`

Options to control schema updates when messages are written to the Snowflake table.

*Type*: `object`

=== `schema_evolution.enabled`

Whether schema evolution is enabled. When set to `true`, the Snowflake table is automatically created based on the schema of the first message written to it, if the table does not already exist. As new fields are added to subsequent messages in the pipeline, new columns are created in the Snowflake table. Any required columns are marked as `nullable` if new messages do not include data for them.

*Type*: `bool`

=== `schema_evolution.ignore_nulls`

When set to `true` and schema evolution is enabled, new columns that have `null` values _are not_ added to the Snowflake table. This behavior:

-  Prevents unnecessary schema changes caused by placeholder or incomplete data.
-  Avoids creating table columns with incorrect data types.

NOTE: Redpanda does not recommend updating the default setting unless you are confident about the data type of `null` columns in advance.

*Type*: `bool`

*Default*: `true`

=== `schema_evolution.processors[]`

A series of processors to execute when new columns are added to the Snowflake table. You can use these processors to:

- Run side effects when the schema evolves.
- Enrich the message with additional information to guide the schema changes.

For example, a processor could read the schema from the schema registry that a message was produced with and use that schema to determine the data type of the new column in Snowflake.

The input to these processors is an object with the value and name of the new column, the original message, and details of the Snowflake table the output writes to. 

For example: `{"value": 42.3, "name":"new_data_field", "message": {"existing_data_field": 42, "new_data_field": "db_field_name"}, "db": MY_DATABASE", "schema": "MY_SCHEMA", "table": "MY_TABLE"}`

The output from the processors must be a valid message, which contains a string that specifies the column type for the new column in Snowflake. The metadata remains the same as in the original message that triggered the schema update.

*Type*: `processor`

[source,yaml]
----
# Examples:
processors:
  - mapping: |-
      root = match this.value.type() {
        this == "string" => "STRING"
        this == "bytes" => "BINARY"
        this == "number" => "DOUBLE"
        this == "bool" => "BOOLEAN"
        this == "timestamp" => "TIMESTAMP"
        _ => "VARIANT"
      }

----

=== `table`

The Snowflake table you want to write data to. This field supports xref:configuration:interpolation.adoc#bloblang-queries[interpolation functions].

*Type*: `string`

[source,yaml]
----
# Examples:
table: MY_TABLE
----

=== `url`

Specify a custom URL to connect to Snowflake. This parameter overrides the default URL, which is automatically generated from the value of `output.snowflake_streaming.account`. By default, the URL is constructed as follows: `https://<output.snowflake_streaming.account>.snowflakecomputing.com`.

*Type*: `string`

[source,yaml]
----
# Examples:
url: https://org-account.privatelink.snowflakecomputing.com
----

=== `user`

Specify a user to run the Snowpipe Stream. To learn how to create a user, see the https://docs.snowflake.com/en/user-guide/admin-user-management[Snowflake documentation^].

*Type*: `string`


