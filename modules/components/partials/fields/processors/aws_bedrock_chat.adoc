// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1247543314/Generate+reference+docs+for+Redpanda+Connect

== Fields

=== `credentials`

Configure which AWS credentials to use (optional). For more information, see xref:guides:cloud/aws.adoc[].

*Type*: `object`

=== `credentials.from_ec2_role`

Use the credentials of a host EC2 machine configured to assume https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html[an IAM role associated with the instance^].

ifndef::env-cloud[]
Requires version 4.2.0 or later.
endif::[]

*Type*: `bool`

=== `credentials.id`

The ID of credentials to use.

*Type*: `string`

=== `credentials.profile`

The profile from `~/.aws/credentials` to use.

*Type*: `string`

=== `credentials.role`

The role ARN to assume.

*Type*: `string`

=== `credentials.role_external_id`

The external ID to use when assuming a role.

*Type*: `string`

=== `credentials.secret`

The secret for the credentials you want to use.

include::redpanda-connect:components:partial$secret_warning.adoc[]

*Type*: `string`

=== `credentials.token`

The token for the credentials you want to use. You must enter this value when using short-term credentials.

*Type*: `string`

=== `endpoint`

A custom endpoint URL for AWS API requests. Use this to connect to AWS-compatible services or local testing environments instead of the standard AWS endpoints.

*Type*: `string`

=== `max_tokens`

The maximum number of tokens to allow in the generated response.

*Type*: `int`

=== `model`

The model ID to use. For a full list, see the https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html[AWS Bedrock documentation^].

*Type*: `string`

[source,yaml]
----
# Examples:
model: amazon.titan-text-express-v1
model: anthropic.claude-3-5-sonnet-20240620-v1:0
model: cohere.command-text-v14
model: meta.llama3-1-70b-instruct-v1:0
model: mistral.mistral-large-2402-v1:0
----

=== `prompt`

The prompt you want to generate a response for. By default, the processor submits the entire payload as a string.

*Type*: `string`

=== `region`

The AWS region to target.

*Type*: `string`

=== `stop[]`

The likelihood of the model selecting higher-probability options while generating a response. A lower value makes the model more likely to choose higher-probability options. A higher value makes the model more likely to choose lower-probability options.

*Type*: `array`

=== `system_prompt`

The system prompt to submit to the AWS Bedrock LLM.

*Type*: `string`

=== `temperature`

A list of stop sequences. A stop sequence is a sequence of characters that causes the model to stop generating the response.

*Type*: `float`

=== `top_p`

The percentage of most-likely candidates that the model considers for the next token. For example, if you choose a value of `0.8`, the model selects from the top 80% of the probability distribution of tokens that could be next in the sequence.

*Type*: `float`


