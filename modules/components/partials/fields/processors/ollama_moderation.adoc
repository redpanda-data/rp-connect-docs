// This content is autogenerated. Do not edit manually. To override descriptions, use the doc-tools CLI with the --overrides option: https://redpandadata.atlassian.net/wiki/spaces/DOC/pages/1247543314/Generate+reference+docs+for+Redpanda+Connect

== Fields

=== `cache_directory`

If the `server_address` is not set, download the Ollama binary to this directory and use it as a model cache.

*Type*: `string`

[source,yaml]
----
# Examples:
cache_directory: /opt/cache/connect/ollama
----

=== `download_url`

If `server_address` is not set, download the Ollama binary from this URL. The default value is the official Ollama GitHub release for this platform.

*Type*: `string`

=== `model`

The name of the Ollama LLM to use.

*Type*: `string`

[cols="1m,2a"]
|===
|Option |Summary

|llama-guard3
|When using llama-guard3, two pieces of metadata is added: @safe with the value of `yes` or `no` and the second being @category for the safety category violation. For more information see the https://ollama.com/library/llama-guard3[Llama Guard 3 Model Card].

|shieldgemma
|When using shieldgemma, the model output is a single piece of metadata of @safe with a value of `yes` or `no` if the response is not in violation of its defined safety policies.

|===

[source,yaml]
----
# Examples:
model: llama-guard3
model: shieldgemma
----

=== `prompt`

The prompt you used to generate a response from an LLM.

If you're using the `ollama_chat` processor, you can set the `save_prompt_metadata` field to save the contents of your prompts. You can then run them through `ollama_moderation` processor to check the model responses for safety. For more details, see <<Examples, Examples>>.

You can also check the safety of your prompts. For more information, see the xref:components:processors/ollama_chat.adoc#examples[`ollama_chat` processor] documentation.



*Type*: `string`

=== `response`

The LLM's response that you want to check for safety.



*Type*: `string`

=== `runner`

Options for the model runner that are used when the model is first loaded into memory.

*Type*: `object`

=== `runner.batch_size`

The maximum number of requests to process in parallel.

*Type*: `int`

=== `runner.context_size`

Sets the size of the context window used to generate the next token. Using a larger context window uses more memory and takes longer to process.

*Type*: `int`

=== `runner.gpu_layers`

Sets the number of layers to offload to the GPU for computation. This generally results in increased performance. By default, the runtime decides the number of layers dynamically.

*Type*: `int`

=== `runner.threads`

Sets the number of threads to use during response generation. For optimal performance, set this value to the number of physical CPU cores your system has. By default, the runtime decides the optimal number of threads.

*Type*: `int`

=== `runner.use_mmap`

Map the model into memory. Set to `true` to load only the necessary parts of the model into memory. This setting is only supported on Unix systems.

*Type*: `bool`

=== `server_address`

The address of the Ollama server to use. Leave this field blank and the processor starts and runs a local Ollama server, or specify the address of your own local or remote server.

*Type*: `string`

[source,yaml]
----
# Examples:
server_address: http://127.0.0.1:11434
----


