= Ingest data into Snowflake
:description: How to configure Redpanda Connect to ingest data from Redpanda into Snowflake using Snowpipe Streaming.

// tag::single-source[]

ifndef::env-cloud[]

In this cookbook we'll use Redpanda Connect to both generate data into a local Redpanda topic, and then take that data and ingest it into
https://www.snowflake.com/en/[Snowflake^] using https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview[Snowpipe Streaming^].

== Prerequisites

- https://docs.redpanda.com/current/get-started/rpk-install/[RPK installed]
- A https://trial.snowflake.com/[Snowflake account^].
- `openssl` command line tool

== Setup your Redpanda cluster

We'll use https://docs.redpanda.com/current/reference/rpk/rpk-container/rpk-container-start/[rpk container start] to create a local cluster for development.

```
rpk container start
```

endif::[]
ifdef::env-cloud[]

In this cookbook we'll use Redpanda Connect to both generate data into a Redpanda Serverless topic, and then take that data and ingest it into
[Snowflake](https://www.snowflake.com/en/) using [Snowpipe Streaming](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview).

== Prerequisites

- A https://cloud.redpanda.com/sign-up[Redpanda Cloud account]
- https://docs.redpanda.com/current/get-started/rpk-install/[RPK installed] and https://docs.redpanda.com/redpanda-cloud/reference/rpk/rpk-cloud/rpk-cloud-login/[signed into your Cloud account]
- A https://trial.snowflake.com/[Snowflake account^].
- `openssl` command line tool

== Setup your Redpanda cluster

Navigate to https://cloud.redpanda.com/[Redpanda Cloud] and create a new serverless cluster. Once that's completed run `rpk cloud cluster select` to select the cluster
and set it to be your current https://docs.redpanda.com/current/get-started/config-rpk-profile/[rpk profile].

endif::[]

Once our cluster is up and running, we can create a topic that we'll use to send data into Snowflake:

[source,bash]
----
rpk topic create demo_topic
----

Next up, we'll create a user with minimal https://docs.redpanda.com/current/manage/security/authorization/acl/[ACLs] to run our ingestion pipeline into Snowflake.

[source,bash]
----
rpk security user create ingestion_user --password Testing1234
----

Now that our user exists, we'll give them read permissions to `demo_topic`, as well as full control over any consumer group with the prefix `redpanda_connect`

[source,bash]
----
rpk security acl create --allow-principal ingestion_user --operation read --topic demo_topic
rpk security acl create --allow-principal ingestion_user --resource-pattern-type prefixed --operation all --group redpanda_connect
----

== Setup your Snowflake account

Login to your Snowflake account with a user who has ACCOUNTADMIN role. You can then run these commands to setup another user with minimal permissions
to write data into a specified database and schema for streaming data. Execute the following SQL commands in a worksheet.

[source,sql]
----
-- Set default value for multiple variables
SET PWD = 'Test1234567';
SET USER = 'STREAMING_USER';
SET DB = 'STREAMING_DB';
SET ROLE = 'REDPANDA_CONNECT';
SET WH = 'STREAMING_WH';
USE ROLE ACCOUNTADMIN;
-- CREATE USERS
CREATE USER IF NOT EXISTS IDENTIFIER($USER) PASSWORD=$PWD  COMMENT='STREAMING USER FOR REDPANDA CONNECT';
-- CREATE ROLES
CREATE OR REPLACE ROLE IDENTIFIER($ROLE);
-- CREATE DATABASE AND WAREHOUSE
CREATE DATABASE IF NOT EXISTS IDENTIFIER($DB);
USE IDENTIFIER($DB);
CREATE OR REPLACE WAREHOUSE IDENTIFIER($WH) WITH WAREHOUSE_SIZE = 'SMALL';
-- GRANTS
GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE IDENTIFIER($ROLE);
GRANT ROLE IDENTIFIER($ROLE) TO USER IDENTIFIER($USER);
GRANT OWNERSHIP ON DATABASE IDENTIFIER($DB) TO ROLE IDENTIFIER($ROLE);
GRANT USAGE ON WAREHOUSE IDENTIFIER($WH) TO ROLE IDENTIFIER($ROLE);
-- SET DEFAULTS
ALTER USER IDENTIFIER($USER) SET DEFAULT_ROLE=$ROLE;
ALTER USER IDENTIFIER($USER) SET DEFAULT_WAREHOUSE=$WH;
-- RUN FOLLOWING COMMANDS TO FIND YOUR ACCOUNT IDENTIFIER, COPY IT DOWN FOR USE LATER
-- IT WILL BE SOMETHING LIKE organization_name-account_name
-- e.g. ykmxgak-wyb52636
WITH HOSTLIST AS 
(SELECT * FROM TABLE(FLATTEN(INPUT => PARSE_JSON(SYSTEM$allowlist()))))
SELECT REPLACE(VALUE:host,'.snowflakecomputing.com','') AS ACCOUNT_IDENTIFIER
FROM HOSTLIST
WHERE VALUE:type = 'SNOWFLAKE_DEPLOYMENT_REGIONLESS';
----

=== Create a RSA key pair

Next up we will need to create an https://docs.snowflake.com/en/user-guide/key-pair-auth[RSA key pair^] using `openssl`
to authenticate Redpanda Connect to Snowflake. You will be prompted to give an encryption password, remember this phrase,
you will need it later.

[source,bash]
----
openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -passout pass:Testing123 -out rsa_key.p8
----

Next we will create a public key by running following commands. You will be prompted to type in the phrase you used in the above step.

[source,bash]
----
openssl rsa -in rsa_key.p8 -pubout -passout pass:Testing123 -out rsa_key.pub
----

To register our public key in Snowflake, we'll need to remove the public key delimiters and output only the base64 encoded portion of the
PEM file. You can print that with the following bash command:

[source,bash]
----
cat rsa_key.pub | sed -e '1d' -e '$d' | tr -d '\n'
----

Now going back to our Snowflake worksheet, we'll enter the output of the above command in the following SQL command and execute.

[source,sql]
----
use role accountadmin;
alter user streaming_user set rsa_public_key='< PubKeyWithoutDelimiters >';
----

=== Create a schema using `streaming_user`

Now logout of Snowflake, sign back in as the default user `streaming_user` we just created with the associated password (default: Test1234567).
Run the following SQL commands in a worksheet to create a schema (e.g. `STREAMING_SCHEMA`) in the default database (e.g. `STREAMING_DB`):

[source,sql]
----
SET DB = 'STREAMING_DB';
SET SCHEMA = 'STREAMING_SCHEMA';
USE IDENTIFIER($DB);
CREATE OR REPLACE SCHEMA IDENTIFIER($SCHEMA);
----

== Create a pipeline from Redpanda to Snowflake

ifndef::env-cloud[]

With all of our setup out of the way, we're now ready to create our pipeline. We'll run our pipeline locally
and inject the xref:configuration:secrets.adoc[secrets] using environment variables. First we need to create
a `connect.yaml` file the following contents.

[source,yaml]
----
input:
  # Read data from our local `demo_topic`
  kafka_franz:
    seed_brokers: ["localhost:9092"]
    topics: ["demo_topic"]
    consumer_group: "redpanda_connect_to_snowflake"
    tls: {enabled: true}
    checkpoint_limit: 4096
    sasl:
      - mechanism: SCRAM-SHA-256
        username: ingestion_user
        password: ${REDPANDA_PASS}
    # Define our batching policy. For this cookbook, we'll create small batches
    # but in a production environment we'll want to create as large of files as
    # possible.
    batching:
      count: 100 # Collect 10 messages before flushing
      period: 10s # or after 10 seconds, which ever comes first
output:
  snowflake_streaming:
    # Make sure to replace this with your account identifier
    account: "< OrgName-AccountName >"
    user: STREAMING_USER
    role: REDPANDA_CONNECT
    database: STREAMING_DB
    schema: STREAMING_SCHEMA
    table: STREAMING_TABLE
    # Inject our private key and password
    private_key_file: rsa_key.pub
    private_key_pass: "${SNOWFLAKE_KEY_PASS}"
    schema_evolution:
      enabled: true
    max_in_flight: 1
----

With the file created, we can now run our pipeline and any JSON data produced into our topic
will now be streamed into Snowflake with minimal latency.

[source,bash]
----
REDPANDA_PASS=Testing1234 SNOWFLAKE_KEY_PASS=Testing123 rpk connect run ./connect.yaml
----

endif::[]
ifdef::env-cloud[]

With all of our setup out of the way, we're now ready to create our pipeline. First thing we need to do is
create xref:configuration:secret-management.adoc[secrets] for the passwords and keys we created during setup.

Navigate to the Connect page in Redpanda Console, then click on the "Secrets" tab and create 3 secrets:

* `REDPANDA_PASS` with the value `Testing1234`
* `SNOWFLAKE_KEY` with the value being the output of `awk '{printf "%s\\n", $0}' rsa_key.p8`
* `SNOWFLAKE_KEY_PASS` with the value `Testing123`

With those secrets created, we can go to the "Pipelines" tab and create a pipeline called 
"RedpandaToSnowflake" and use the following YAML configuration.

[source,yaml]
----
input:
  # Read data from our `demo_topic`
  kafka_franz:
    seed_brokers: ["${REDPANDA_BROKERS}"]
    topics: ["demo_topic"]
    consumer_group: "redpanda_connect_to_snowflake"
    tls: {enabled: true}
    checkpoint_limit: 4096
    sasl:
      - mechanism: SCRAM-SHA-256
        username: ingestion_user
        password: ${secrets.REDPANDA_PASS}
    # Define our batching policy. For this cookbook, we'll create small batches
    # but in a production environment we'll want to create as large of files as
    # possible.
    batching:
      count: 100 # Collect 10 messages before flushing
      period: 10s # or after 10 seconds, which ever comes first
output:
  snowflake_streaming:
    # Make sure to replace this with your account identifier
    account: "< OrgName-AccountName >"
    user: STREAMING_USER
    role: REDPANDA_CONNECT
    database: STREAMING_DB
    schema: STREAMING_SCHEMA
    table: STREAMING_TABLE
    # Inject our private key and password
    private_key_file: "${secrets.SNOWFLAKE_KEY}"
    private_key_pass: "${secrets.SNOWFLAKE_KEY_PASS}"
    schema_evolution:
      enabled: true
    max_in_flight: 1
----

endif::[]

With our pipeline running, you can produce some data using `rpk` to test that things are working:

[source,bash]
----
echo '{"animal":"redpanda","attributes":"cute","age":6}' | rpk topic produce demo_topic -f '%v\n'
echo '{"animal":"polar bear","attributes":"cool","age":13}' | rpk topic produce demo_topic -f '%v\n'
echo '{"animal":"unicorn","attributes":"rare","age":999}' | rpk topic produce demo_topic -f '%v\n'
----

With the data produced into the topic, it will be consumed and streamed into Snowflake on the order of seconds.
We can now go back to our Snowflake worksheet and run the following query to see data showing up live, with the
schema from the JSON data we produced.

[source,sql]
----
SELECT * FROM STREAMING_DB.STREAMING_SCHEMA.STREAMING_DATA LIMIT 50;
----

