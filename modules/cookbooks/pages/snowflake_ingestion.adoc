= Ingest data into Snowflake
:description: Configure Redpanda Connect to ingest data from Redpanda into Snowflake using Snowpipe Streaming.

// tag::single-source[]

ifndef::env-cloud[]

In this cookbook, we use Redpanda Connect to generate data into a local Redpanda topic and then ingest that data into
https://www.snowflake.com/en/[Snowflake^] using https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview[Snowpipe Streaming^].

== Prerequisites

- https://docs.redpanda.com/current/get-started/rpk-install/[`rpk` installed]
- A https://trial.snowflake.com/[Snowflake account^].
- `openssl` command line tool

== Setup your Redpanda cluster

Run https://docs.redpanda.com/current/reference/rpk/rpk-container/rpk-container-start/[rpk container start] to create a local cluster for development:

```
rpk container start
```

endif::[]
ifdef::env-cloud[]

In this cookbook, we use Redpanda Connect to generate data into a Redpanda Serverless topic, and then ingest that data into
[Snowflake](https://www.snowflake.com/en/) using [Snowpipe Streaming](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-overview).

== Prerequisites

- A https://cloud.redpanda.com/sign-up[Redpanda Cloud account]
- https://docs.redpanda.com/current/get-started/rpk-install/[`rpk` installed] and https://docs.redpanda.com/redpanda-cloud/reference/rpk/rpk-cloud/rpk-cloud-login/[signed into your Cloud account]
- A https://trial.snowflake.com/[Snowflake account^].
- `openssl` command line tool

== Set up your Redpanda cluster

In https://cloud.redpanda.com/[Redpanda Cloud], create a new Serverless Standard cluster. After that completes, run `rpk cloud cluster select` to select the cluster
and set it to be your current https://docs.redpanda.com/current/get-started/config-rpk-profile/[rpk profile].

endif::[]

After the cluster is running, create a topic to send data into Snowflake:

[source,bash]
----
rpk topic create demo_topic
----

Create a user with minimal https://docs.redpanda.com/current/manage/security/authorization/acl/[ACLs] to run the ingestion pipeline into Snowflake:

[source,bash]
----
rpk security user create ingestion_user --password Testing1234
----

Now that the user exists, give them read permissions to `demo_topic`, as well as full control over any consumer group with the prefix `redpanda_connect`:

[source,bash]
----
rpk security acl create --allow-principal ingestion_user --operation read --topic demo_topic
rpk security acl create --allow-principal ingestion_user --resource-pattern-type prefixed --operation all --group redpanda_connect
----

== Set up your Snowflake account

Log in to your Snowflake account with a user who has ACCOUNTADMIN role. You can then run these commands to set up another user with minimal permissions
to write data into a specified database and schema for streaming data. Run the following SQL commands in a worksheet:

[source,sql]
----
-- Set default value for multiple variables
SET PWD = 'Test1234567';
SET USER = 'STREAMING_USER';
SET DB = 'STREAMING_DB';
SET ROLE = 'REDPANDA_CONNECT';
SET WH = 'STREAMING_WH';
USE ROLE ACCOUNTADMIN;
-- CREATE USERS
CREATE USER IF NOT EXISTS IDENTIFIER($USER) PASSWORD=$PWD  COMMENT='STREAMING USER FOR REDPANDA CONNECT';
-- CREATE ROLES
CREATE OR REPLACE ROLE IDENTIFIER($ROLE);
-- CREATE DATABASE AND WAREHOUSE
CREATE DATABASE IF NOT EXISTS IDENTIFIER($DB);
USE IDENTIFIER($DB);
CREATE OR REPLACE WAREHOUSE IDENTIFIER($WH) WITH WAREHOUSE_SIZE = 'SMALL';
-- GRANTS
GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE IDENTIFIER($ROLE);
GRANT ROLE IDENTIFIER($ROLE) TO USER IDENTIFIER($USER);
GRANT OWNERSHIP ON DATABASE IDENTIFIER($DB) TO ROLE IDENTIFIER($ROLE);
GRANT USAGE ON WAREHOUSE IDENTIFIER($WH) TO ROLE IDENTIFIER($ROLE);
-- SET DEFAULTS
ALTER USER IDENTIFIER($USER) SET DEFAULT_ROLE=$ROLE;
ALTER USER IDENTIFIER($USER) SET DEFAULT_WAREHOUSE=$WH;
-- RUN FOLLOWING COMMANDS TO FIND YOUR ACCOUNT IDENTIFIER, COPY IT DOWN FOR USE LATER
-- IT WILL BE SOMETHING LIKE organization_name-account_name
-- e.g. ykmxgak-wyb52636
WITH HOSTLIST AS 
(SELECT * FROM TABLE(FLATTEN(INPUT => PARSE_JSON(SYSTEM$allowlist()))))
SELECT REPLACE(VALUE:host,'.snowflakecomputing.com','') AS ACCOUNT_IDENTIFIER
FROM HOSTLIST
WHERE VALUE:type = 'SNOWFLAKE_DEPLOYMENT_REGIONLESS';
----

=== Create a RSA key pair

Create an https://docs.snowflake.com/en/user-guide/key-pair-auth[RSA key pair^] using `openssl`
to authenticate Redpanda Connect to Snowflake. You are prompted to give an encryption password. Remember this phrase:
you will need it later.

[source,bash]
----
openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -passout pass:Testing123 -out rsa_key.p8
----

Create a public key. You are prompted to enter your encryption password.

[source,bash]
----
openssl rsa -in rsa_key.p8 -pubout -passout pass:Testing123 -out rsa_key.pub
----

To register the public key in Snowflake, remove the public key delimiters and output only the base64 encoded portion of the
PEM file. You can print that with the following bash command:

[source,bash]
----
cat rsa_key.pub | sed -e '1d' -e '$d' | tr -d '\n'
----

Back to the Snowflake worksheet, enter the output of the above command in the following SQL command and run:

[source,sql]
----
use role accountadmin;
alter user streaming_user set rsa_public_key='< PubKeyWithoutDelimiters >';
----

=== Create a schema using `streaming_user`

Log out of Snowflake, then sign back in as the default user `streaming_user` you just created with the associated password (default: Test1234567).
Run the following SQL commands in a worksheet to create a schema (e.g., `STREAMING_SCHEMA`) in the default database (e.g., `STREAMING_DB`):

[source,sql]
----
SET DB = 'STREAMING_DB';
SET SCHEMA = 'STREAMING_SCHEMA';
USE IDENTIFIER($DB);
CREATE OR REPLACE SCHEMA IDENTIFIER($SCHEMA);
----

== Create a pipeline from Redpanda to Snowflake

ifndef::env-cloud[]

You can now create the pipeline. Run the pipeline locally
and inject the xref:configuration:secrets.adoc[secrets] using environment variables. First, create
a `connect.yaml` file:

[source,yaml]
----
input:
  # Read data from our local `demo_topic`
  kafka_franz:
    seed_brokers: ["localhost:9092"]
    topics: ["demo_topic"]
    consumer_group: "redpanda_connect_to_snowflake"
    tls: {enabled: true}
    checkpoint_limit: 4096
    sasl:
      - mechanism: SCRAM-SHA-256
        username: ingestion_user
        password: ${REDPANDA_PASS}
    # Define our batching policy. For this cookbook, we create small batches,
    # but a production environment should create files as large as possible.
    batching:
      count: 100 # Collect 10 messages before flushing
      period: 10s # or after 10 seconds, which ever comes first
output:
  snowflake_streaming:
    # Make sure to replace this with your account identifier
    account: "< OrgName-AccountName >"
    user: STREAMING_USER
    role: REDPANDA_CONNECT
    database: STREAMING_DB
    schema: STREAMING_SCHEMA
    table: STREAMING_TABLE
    # Inject our private key and password
    private_key_file: rsa_key.pub
    private_key_pass: "${SNOWFLAKE_KEY_PASS}"
    schema_evolution:
      enabled: true
    max_in_flight: 1
----

With the file created, you can now run the pipeline, and any JSON data produced into the topic
is streamed into Snowflake with minimal latency.

[source,bash]
----
REDPANDA_PASS=Testing1234 SNOWFLAKE_KEY_PASS=Testing123 rpk connect run ./connect.yaml
----

endif::[]
ifdef::env-cloud[]

You can now create the pipeline. First create xref:configuration:secret-management.adoc[secrets] for the passwords and keys you created during setup.

Navigate to the Connect page in Redpanda Console, then click the "Secrets" tab, and create three secrets:

* `REDPANDA_PASS` with the value `Testing1234`
* `SNOWFLAKE_KEY` with the value being the output of `awk '{printf "%s\\n", $0}' rsa_key.p8`
* `SNOWFLAKE_KEY_PASS` with the value `Testing123`

Go to the "Pipelines" tab and create a pipeline called 
"RedpandaToSnowflake". Use the following YAML configuration:

[source,yaml]
----
input:
  # Read data from our `demo_topic`
  kafka_franz:
    seed_brokers: ["${REDPANDA_BROKERS}"]
    topics: ["demo_topic"]
    consumer_group: "redpanda_connect_to_snowflake"
    tls: {enabled: true}
    checkpoint_limit: 4096
    sasl:
      - mechanism: SCRAM-SHA-256
        username: ingestion_user
        password: ${secrets.REDPANDA_PASS}
    # Define our batching policy. For this cookbook, we'll create small batches
    # but in a production environment we'll want to create as large of files as
    # possible.
    batching:
      count: 100 # Collect 10 messages before flushing
      period: 10s # or after 10 seconds, which ever comes first
output:
  snowflake_streaming:
    # Make sure to replace this with your account identifier
    account: "< OrgName-AccountName >"
    user: STREAMING_USER
    role: REDPANDA_CONNECT
    database: STREAMING_DB
    schema: STREAMING_SCHEMA
    table: STREAMING_TABLE
    # Inject our private key and password
    private_key_file: "${secrets.SNOWFLAKE_KEY}"
    private_key_pass: "${secrets.SNOWFLAKE_KEY_PASS}"
    schema_evolution:
      enabled: true
    max_in_flight: 1
----

endif::[]

With our pipeline running, you can produce some data using `rpk` to test that things are working:

[source,bash]
----
echo '{"animal":"redpanda","attributes":"cute","age":6}' | rpk topic produce demo_topic -f '%v\n'
echo '{"animal":"polar bear","attributes":"cool","age":13}' | rpk topic produce demo_topic -f '%v\n'
echo '{"animal":"unicorn","attributes":"rare","age":999}' | rpk topic produce demo_topic -f '%v\n'
----

With the data produced into the topic, it will be consumed and streamed into Snowflake on the order of seconds.
We can now go back to our Snowflake worksheet and run the following query to see data showing up live, with the
schema from the JSON data we produced.

[source,sql]
----
SELECT * FROM STREAMING_DB.STREAMING_SCHEMA.STREAMING_DATA LIMIT 50;
----

